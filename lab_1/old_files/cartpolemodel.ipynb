{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import gym # pull the environment\n",
    "\n",
    "import time # to get the time\n",
    "\n",
    "import math # needed for calculations\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_discrete_state(state):\n",
    "   \n",
    "    discrete_state = state / np_array_win_size + np.array([15, 10, 1, 10])\n",
    "    #print(state,discrete_state)\n",
    "    return tuple(discrete_state.astype(np.int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 1.825094223022461e-06\n",
      "Mean Reward: 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/p09k60990slfl_mz2vbnm5vw0000gn/T/ipykernel_59704/417261002.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Average: 0.000363034725189209\n",
      "Mean Reward: 22.934\n",
      "Episode: 2000\n",
      "Time Average: 0.0003518097400665283\n",
      "Mean Reward: 22.273\n",
      "Time Average: 0.0003257036209106445\n",
      "Mean Reward: 22.03\n",
      "Episode: 4000\n",
      "Time Average: 0.0003334007263183594\n",
      "Mean Reward: 21.437\n",
      "Time Average: 0.00032359814643859863\n",
      "Mean Reward: 21.931\n",
      "Episode: 6000\n",
      "Time Average: 0.0003158245086669922\n",
      "Mean Reward: 22.259\n",
      "Time Average: 0.00033723783493041994\n",
      "Mean Reward: 22.579\n",
      "Episode: 8000\n",
      "Time Average: 0.00033283782005310056\n",
      "Mean Reward: 22.634\n",
      "Time Average: 0.00033444571495056154\n",
      "Mean Reward: 22.361\n",
      "Episode: 10000\n",
      "Time Average: 0.00033573174476623537\n",
      "Mean Reward: 21.748\n",
      "Time Average: 0.00034241247177124024\n",
      "Mean Reward: 23.505\n",
      "Epsilon: 0.9277417467531685\n",
      "Episode: 12000\n",
      "Time Average: 0.00035610103607177734\n",
      "Mean Reward: 24.151\n",
      "Epsilon: 0.8824941446941661\n",
      "Epsilon: 0.8607047486686201\n",
      "Time Average: 0.0003732798099517822\n",
      "Mean Reward: 25.347\n",
      "Episode: 14000\n",
      "Time Average: 0.00039419984817504883\n",
      "Mean Reward: 27.339\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.0004079158306121826\n",
      "Mean Reward: 28.464\n",
      "Epsilon: 0.7595669010105212\n",
      "Episode: 16000\n",
      "Epsilon: 0.7408126643807126\n",
      "Time Average: 0.00045869898796081543\n",
      "Mean Reward: 31.77\n",
      "Time Average: 0.0004851510524749756\n",
      "Mean Reward: 34.036\n",
      "Epsilon: 0.687282835269431\n",
      "Episode: 18000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.000514958381652832\n",
      "Mean Reward: 35.211\n",
      "Epsilon: 0.6537628386312633\n",
      "Time Average: 0.0005747852325439453\n",
      "Mean Reward: 38.901\n",
      "Epsilon: 0.6218776713776856\n",
      "Episode: 20000\n",
      "Epsilon: 0.606523077874078\n",
      "Time Average: 0.0006557340621948242\n",
      "Mean Reward: 43.85\n",
      "Epsilon: 0.5769418771107269\n",
      "Time Average: 0.0006437644958496093\n",
      "Mean Reward: 44.587\n",
      "Epsilon: 0.5626967797130051\n",
      "Episode: 22000\n",
      "Epsilon: 0.5488034037068503\n",
      "Time Average: 0.0007302823066711425\n",
      "Mean Reward: 49.312\n",
      "Time Average: 0.0007877068519592285\n",
      "Mean Reward: 53.879\n",
      "Epsilon: 0.5091478283790776\n",
      "Episode: 24000\n",
      "Epsilon: 0.4965766133349901\n",
      "Time Average: 0.0008822977542877197\n",
      "Mean Reward: 59.371\n",
      "Epsilon: 0.47235769565598784\n",
      "Time Average: 0.0009175324440002441\n",
      "Mean Reward: 62.555\n",
      "Episode: 26000\n",
      "Epsilon: 0.44931997732828616\n",
      "Time Average: 0.00098661208152771\n",
      "Mean Reward: 69.045\n",
      "Epsilon: 0.4274058491752072\n",
      "Time Average: 0.0010672414302825928\n",
      "Mean Reward: 73.87\n",
      "Episode: 28000\n",
      "Epsilon: 0.4065605117212756\n",
      "Time Average: 0.0011316442489624024\n",
      "Mean Reward: 77.689\n",
      "Epsilon: 0.396522249086328\n",
      "Epsilon: 0.3867318381581326\n",
      "Time Average: 0.00119785475730896\n",
      "Mean Reward: 82.866\n",
      "Episode: 30000\n",
      "Time Average: 0.001294187068939209\n",
      "Mean Reward: 88.901\n",
      "Time Average: 0.0013383586406707764\n",
      "Mean Reward: 93.885\n",
      "Episode: 32000\n",
      "Epsilon: 0.33286192956404903\n",
      "Time Average: 0.0015198118686676025\n",
      "Mean Reward: 104.821\n",
      "Epsilon: 0.32464333633178233\n",
      "Epsilon: 0.31662766589938623\n",
      "Time Average: 0.0016695561408996583\n",
      "Mean Reward: 114.647\n",
      "Episode: 34000\n",
      "Epsilon: 0.3011851759202241\n",
      "Time Average: 0.0015814478397369386\n",
      "Mean Reward: 109.255\n",
      "Epsilon: 0.29374870383187524\n",
      "Time Average: 0.001816399335861206\n",
      "Mean Reward: 125.01\n",
      "Epsilon: 0.27942206120438906\n",
      "Episode: 36000\n",
      "Time Average: 0.0017551388740539551\n",
      "Mean Reward: 121.282\n",
      "Epsilon: 0.25923151114313064\n",
      "Time Average: 0.002029097080230713\n",
      "Mean Reward: 142.408\n",
      "Episode: 38000\n",
      "Epsilon: 0.24658833291124824\n",
      "Time Average: 0.002011857271194458\n",
      "Mean Reward: 140.66\n",
      "Epsilon: 0.24049989496139146\n",
      "Time Average: 0.0022475152015686037\n",
      "Mean Reward: 155.923\n",
      "Episode: 40000\n",
      "Time Average: 0.002216332197189331\n",
      "Mean Reward: 155.682\n",
      "Time Average: 0.0020652761459350587\n",
      "Mean Reward: 144.541\n",
      "Epsilon: 0.206999401647574\n",
      "Episode: 42000\n",
      "Time Average: 0.0022451176643371582\n",
      "Mean Reward: 155.895\n",
      "Epsilon: 0.19690367556326213\n",
      "Time Average: 0.002434078216552734\n",
      "Mean Reward: 170.556\n",
      "Episode: 44000\n",
      "Epsilon: 0.18267575990917997\n",
      "Time Average: 0.0020162551403045655\n",
      "Mean Reward: 140.437\n",
      "Epsilon: 0.17816536796962992\n",
      "Time Average: 0.002295283317565918\n",
      "Mean Reward: 158.819\n",
      "Episode: 46000\n",
      "Time Average: 0.0027030880451202393\n",
      "Mean Reward: 188.436\n",
      "Epsilon: 0.16121028849740862\n",
      "Epsilon: 0.15722989402047993\n",
      "Time Average: 0.0025082719326019287\n",
      "Mean Reward: 173.296\n",
      "Epsilon: 0.15334777825975254\n",
      "Episode: 48000\n",
      "Epsilon: 0.1495615146451681\n",
      "Time Average: 0.0024748764038085938\n",
      "Mean Reward: 172.228\n",
      "Epsilon: 0.1422671356634204\n",
      "Time Average: 0.002886109113693237\n",
      "Mean Reward: 202.586\n",
      "Epsilon: 0.13875446084395782\n",
      "Episode: 50000\n",
      "Time Average: 0.0029135286808013917\n",
      "Mean Reward: 204.877\n",
      "Epsilon: 0.13198716094595161\n",
      "Time Average: 0.002994673728942871\n",
      "Mean Reward: 207.982\n",
      "Epsilon: 0.12554991420537906\n",
      "Episode: 52000\n",
      "Epsilon: 0.12244999924498873\n",
      "Time Average: 0.0028796346187591553\n",
      "Mean Reward: 200.484\n",
      "Epsilon: 0.11647789670960881\n",
      "Time Average: 0.003007716178894043\n",
      "Mean Reward: 210.473\n",
      "Episode: 54000\n",
      "Epsilon: 0.11079706415310193\n",
      "Time Average: 0.0032129168510437013\n",
      "Mean Reward: 226.139\n",
      "Time Average: 0.002725767135620117\n",
      "Mean Reward: 190.784\n",
      "Episode: 56000\n",
      "Epsilon: 0.10025307881289336\n",
      "Time Average: 0.003263669490814209\n",
      "Mean Reward: 230.435\n",
      "Epsilon: 0.09777776036441635\n",
      "Time Average: 0.0033481614589691163\n",
      "Mean Reward: 233.805\n",
      "Epsilon: 0.09300896645525675\n",
      "Episode: 58000\n",
      "Time Average: 0.003435434341430664\n",
      "Mean Reward: 239.737\n",
      "Epsilon: 0.08847275503994115\n",
      "Time Average: 0.0037546069622039794\n",
      "Mean Reward: 262.784\n",
      "Epsilon: 0.08415778265983555\n",
      "Episode: 60000\n",
      "Epsilon: 0.08207986830082019\n",
      "Time Average: 0.0037794034481048585\n",
      "Mean Reward: 262.613\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES + 1): \n",
    "    t0 = time.time() \n",
    "    discrete_state = get_discrete_state(env.reset()[0])\n",
    "    done = False\n",
    "    episode_reward = 0 \n",
    "\n",
    "    #print(episode)\n",
    " \n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "        \n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) \n",
    "            \n",
    "        else:\n",
    "           \n",
    "            action = np.random.randint(0, env.action_space.n) \n",
    "\n",
    "       \n",
    "        new_state, reward, done, nil , _ = env.step(action) \n",
    "\n",
    "        episode_reward += reward \n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        \n",
    "        if episode % 2000 == 0: \n",
    "            env.render()\n",
    "\n",
    "        \n",
    "           \n",
    "\n",
    "        if not done: \n",
    "            \n",
    "            \n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: \n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    t1 = time.time() \n",
    "    episode_total = t1 - t0 \n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward \n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: \n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rand_policy_agent(observation):\n",
    "    return random.randint(0, 1)\n",
    "     \n",
    "\n",
    "totalreward = 0\n",
    "# Create CartPole-v1 environment\n",
    "env = gym.make('CartPole-v1',render_mode='rgb_array')\n",
    "\n",
    "# Reset environment\n",
    "observation = env.reset()[0]\n",
    "discrete_state = get_discrete_state(observation)\n",
    "# Display the environment\n",
    "plt.figure(figsize=(9, 3))\n",
    "img = plt.imshow(np.zeros((400, 600, 3)))\n",
    "\n",
    "# Loop to take random actions until episode ends\n",
    "done = False\n",
    "while not done:\n",
    "# Render the environment\n",
    "    screen = env.render()\n",
    "    \n",
    "    action = np.argmax(q_table[discrete_state]) \n",
    "    \n",
    "\n",
    "    observation, reward, done,nil, _ = env.step(action)\n",
    "    discrete_state = get_discrete_state(observation)\n",
    "\n",
    "    totalreward += reward\n",
    "    \n",
    "    img.set_data(screen)\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "# Close the environment\n",
    "print(totalreward)\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/p09k60990slfl_mz2vbnm5vw0000gn/T/ipykernel_59704/417261002.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/weihong/Desktop/weihong/AI/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/weihong/Desktop/weihong/AI/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/weihong/Desktop/weihong/AI/video/rl-video-episode-0.mp4\n",
      "657.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/p09k60990slfl_mz2vbnm5vw0000gn/T/ipykernel_59704/417261002.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "     \n",
    "\n",
    "totalreward = 0\n",
    "\n",
    "\n",
    "env = RecordVideo(gym.make('CartPole-v1',render_mode='rgb_array'), \"./video\",)\n",
    "observation = env.reset()\n",
    "discrete_state = get_discrete_state(observation[0])\n",
    "\n",
    "while True:\n",
    "    #env.render()\n",
    "    #your agent goes here\n",
    "    action = np.argmax(q_table[discrete_state]) \n",
    "    \n",
    "    observation, reward, done, info, _ = env.step(action) \n",
    "    discrete_state = get_discrete_state(observation)\n",
    "    totalreward += reward\n",
    "\n",
    "    if done: \n",
    "      print(totalreward)\n",
    "      break;    \n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/weihong/Desktop/weihong/AI/video/rl-video-episode-0.mp4']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(f'{env.video_folder}/{env.name_prefix}*.mp4')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/weihong/Desktop/weihong/AI/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962f6a41bdd44041abfecc176d487fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from IPython.display import Video\n",
    "\n",
    "from ipywidgets import Video\n",
    "\n",
    "print(files[0])\n",
    "Video.from_file(files[0], embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
