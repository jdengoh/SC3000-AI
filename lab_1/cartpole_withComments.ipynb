{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc92bb0-7772-4f0d-8173-a379a1f9ddc8",
   "metadata": {},
   "source": [
    "# Balancing a Pole on a Cart\n",
    "## Members\n",
    "\n",
    "Jden Goh\n",
    "\n",
    "Alan Lee\n",
    "\n",
    "Wei Hong\n",
    "\n",
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7412daf-b208-40ce-a479-2e3420cdb487",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbd6fec-df3f-45ac-bada-b25cc15c6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "import gym # pull the environment\n",
    "import time # to get the time\n",
    "import math # needed for calculations\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25540283-a527-41f2-ac2d-55d054dbbee1",
   "metadata": {},
   "source": [
    "# Exploring our Cartpole Environment\n",
    "\n",
    "References:\n",
    "* https://github.com/yue-zhongqi/cartpole_colab/blob/main/cartpole.ipynb\n",
    "* https://www.linkedin.com/pulse/reinforcement-learning-pytorch-mastering-cartpole-v0-akbarnezhad#:~:text=For%20example%2C%20in%20the%20CartPole,as%20observations%20to%20the%20agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89055914-f0b8-4c90-93a3-dcd2c1e20180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Environment\n",
    "test_env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb04306-de2f-4a2b-9d62-808bdd5fb678",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "Action space defines the set of discrete action our agent is allowed to take to interact with the environment.\n",
    "\n",
    "Our cartpole environment has 2 descrete actions possible, `0` and `1`, as seen by the output `Discrete(2)`:\n",
    "\n",
    "* `0` - Left\n",
    "* `1` -> Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824947bf-9d0a-4dbd-b60d-6d6e242e775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908e349-8e46-473e-82a4-07df22c12fb7",
   "metadata": {},
   "source": [
    "## Observation Space\n",
    "Our observation consists of four continuous values:\n",
    "* **Cart Position**\n",
    "* **Cart Velocity**\n",
    "* **Pole Angle**\n",
    "* **Pole Angular Velocity**\n",
    "\n",
    "The *min* and *max* for each corresponding values is defined by the output below, in the same order as listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a716f52-21ac-45c3-b67d-48debba90eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e50a8-e427-429e-bed1-ff0c89d2cf4e",
   "metadata": {},
   "source": [
    "# Q-Learning Agent\n",
    "## Introduction\n",
    "Q=Learning is an off-model, temporal difference policy, where only need **one-step** trajectory. The Q-Learning agent will store the reards of the **current state** in a `q-table`, and the model will find the best series of action to take based on the **current state** of the agent.\n",
    "\n",
    "---\n",
    "\n",
    "## Q-Value\n",
    "**Q-value** computing (based on lecture notes):\n",
    "$${\\huge Q_{new}(S,A) \\leftarrow Q_{old} (S_t , A_t) + \\alpha[R + \\gamma max_a Q(S',a) - Q_{old}(S_t,A_t)]} $$\n",
    "\n",
    "where:\n",
    "* $\\alpha \\rightarrow$ learning rate\n",
    "* $\\gamma \\rightarrow$ discount factor\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation:\n",
    "*note: Our code take reference from this [tutorial](https://www.youtube.com/watch?v=2u1REHeHMrg), we have studeid and understood it, and re-implemented it on our own to ensure that we are able to learn from this tutorial.*\n",
    "\n",
    "### Observation Space\n",
    "We have taken the 4 continuous values (in the observation space) and converted into **bins** of discrete ranges using `np.linspace` function.\n",
    "\n",
    "This is because the CartPole game is **continuous**, while the Q-Learning agent works best in **discrete state space**.\n",
    "\n",
    "Using the `np.linspace` function, we have changed the 4 continous values into discrete bins\n",
    "\n",
    "### Q-table\n",
    "We initialised a `q_table` using `np.zeroes` library. The shape of the library is given by the discrete bins that we have computed earlier, with the inclusion of an additional axis for the action.\n",
    "\n",
    "For our `trainModel` function, we initialise a new `q-table` and save it as a `.pkl` file using the python `pickle` library.\n",
    "\n",
    "We can re-use this `q-table` that we have trained later on for our actual runs.\n",
    "\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "We have initialised the following parameters for our model (based on our lecture):\n",
    "\n",
    "* $\\alpha $ = 0.1\n",
    "* $\\gamma $ = 0.99\n",
    "\n",
    "### Epsilon greedy policy\n",
    "Adopting the **epsilon greedy policy** for this model, we have also intialised the following parameters:\n",
    "* `epsilon` = 1.0\n",
    "* `epsilon_decay_rate` = 0.0001\n",
    "\n",
    "This values help us handle the exploration and exploitation tradeoff. The idea is to decide between:\n",
    "1. Exploring a new random action, *or*\n",
    "2. Explointing what the agent knows\n",
    "\n",
    "In the intial stages of training, the agent relies on trial-and-error to discorver and explore the environment that it has no knowledge on. As the agent learns more and more about the environment, it will then slowly began to exploit its knowledge.\n",
    "\n",
    "This is implemented using the `epsilon` value and a random value:\n",
    "* a random number is generated between 0 and 1\n",
    "* if random number > `epsilon`, we **exploit**\n",
    "* else, we **explore**\n",
    "\n",
    "Intially when `epsilon` = 1.0, the agent will focus heavily on **exploring**, but with the implementation of the **epsilon_decay_rate**, the **epsilon** value will gradually decrease and hence, the agent will slowly began to make decisions based on its learned-knowldege.\n",
    "\n",
    "## References:\n",
    "\n",
    "https://www.youtube.com/watch?v=2u1REHeHMrg\n",
    "\n",
    "https://github.com/JackFurby/CartPole-v0\n",
    "\n",
    "https://towardsdatascience.com/intro-to-reinforcement-learning-temporal-difference-learning-sarsa-vs-q-learning-8b4184bb4978\n",
    "\n",
    "https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b556e2f-b47a-4095-a0a4-9458e24c0be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6a5fd4-9167-4790-a622-58aaad130774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')             # our cartpole environment\n",
    "        self.p_space = np.linspace(-2.4, 2.4, 10)\n",
    "        self.v_space = np.linspace(-4, 4, 10)\n",
    "        self.a_space = np.linspace(-.2095, .2095, 10)\n",
    "        self.av_space = np.linspace(-4, 4, 10)\n",
    "\n",
    "        self.alpha = 0.1                                # Learning-rate\n",
    "        self.gamma = 0.99                               # Discount-factor\n",
    "        self.epsilon = 1                                # Epsilon\n",
    "        self.epsilon_decay_rate = 0.00001               # Epsilon decay factor\n",
    "        \n",
    "        self.is_training = True\n",
    "        self.end = False\n",
    "        self.episode_count = 0\n",
    "        self.reward_episodes = []\n",
    "\n",
    "\n",
    "    def digitise_state(self, state):\n",
    "        \n",
    "        digi_p = np.digitize(state[0],self.p_space)\n",
    "        digi_v= np.digitize(state[1],self.v_space)\n",
    "        digi_a=np.digitize(state[2],self.a_space)\n",
    "        digi_av=np.digitize(state[3],self.av_space)\n",
    "\n",
    "        return[digi_p, digi_v, digi_a, digi_av]\n",
    "\n",
    "    #train agent and update q values\n",
    "    def trainModel(self, desired_mean):\n",
    "       \n",
    "        q_table = np.zeros((len(self.p_space)+1, \n",
    "                            len(self.v_space)+1, \n",
    "                            len(self.a_space)+1, \n",
    "                            len(self.av_space)+1, \n",
    "                            self.env.action_space.n))\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.reward_episodes = []\n",
    "\n",
    "        while(True):            \n",
    "            #initialises the environment\n",
    "            current_state = self.env.reset()[0]       \n",
    "            self.end = False\n",
    "            rewards = 0\n",
    "            \n",
    "            #convert the contininous state into discrete bin values for q table to compute \n",
    "            \n",
    "            digi_state = self.digitise_state(current_state)\n",
    "\n",
    "            \n",
    "            #close loop when either episode ends or rewards exceeds desired threshold\n",
    "            while(not self.end and rewards<1000):\n",
    "                \n",
    "                #generates random number to determine whether to explore or exploit\n",
    "                randomNumber=np.random.random()\n",
    "                if randomNumber < self.epsilon:\n",
    "                    #selecting a random action\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    #selecting action based on q-table\n",
    "                    action = np.argmax(q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3], :])\n",
    "\n",
    "                #executes the action and determines the new state,rewards obtained and whether episode would end\n",
    "                new_state, reward, self.end, _ ,_ = self.env.step(action)\n",
    "                \n",
    "                #convert the next contininous state into discrete bin values for q table to compute \n",
    "                new_digi_state = self.digitise_state(new_state)\n",
    "\n",
    "                                \n",
    "                #updates q table values                \n",
    "                q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3], action] = q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3], action] + self.alpha * (reward + self.gamma * np.max(q_table[new_digi_state[0], new_digi_state[1], new_digi_state[2], new_digi_state[3],:]) - q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3],action])\n",
    "\n",
    "\n",
    "                #prepare next state\n",
    "                current_state = new_state\n",
    "                digi_state = new_digis_state\n",
    "\n",
    "                rewards += reward\n",
    "\n",
    "            #adds the reward obtained in the current episode to list containing rewards obtained in each episode\n",
    "            self.reward_episodes.append(rewards)\n",
    "            \n",
    "            #calcalates mean rewards obtained over the past 100 epsiodes\n",
    "            cumulative_rewards = np.mean(self.reward_episodes[len(self.reward_episodes)-100:])\n",
    "            \n",
    "            \n",
    "            #prints out training updates every 100 episodes\n",
    "            if self.episode_count%100==0:\n",
    "                print(f'Episode: {self.episode_count}: {rewards}  Epsilon: {self.epsilon:0.2f}  Cumulative_Rewards {cumulative_rewards:0.1f}')\n",
    "            \n",
    "            #end training if satisfactory mean rewards values above 500 obtained\n",
    "            if mean_rewards>desired_mean:\n",
    "                break\n",
    "\n",
    "            #decreases exploration rate so agent will lean towards exploitation over time    \n",
    "            self.epsilon = max(self.epsilon - self.epsilon_decay_rate, 0)\n",
    "            \n",
    "            #increment training episode\n",
    "            self.episode_count+=1\n",
    "\n",
    "        #close environment\n",
    "        self.env.close()\n",
    "        \n",
    "        #saves q table\n",
    "        if self.is_training:\n",
    "            f = open('cartpole.pkl','wb')\n",
    "            pickle.dump(q_table, f)\n",
    "            f.close()\n",
    "    \n",
    "    #simulate episodes without updating q values\n",
    "    def simulateEpisodes(self, no_of_episodes):\n",
    "        \n",
    "        #create environment\n",
    "        self.env = gym.make('CartPole-v1', render_mode='human')\n",
    "        \n",
    "        #loads q table\n",
    "        f = open('cartpole.pkl', 'rb')\n",
    "        q_table = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.reward_episodes = []\n",
    " \n",
    "        for i in range(no_of_episodes):\n",
    "  \n",
    "            #initalises the environment\n",
    "            current_state = self.env.reset()[0]       \n",
    "            self.end = False\n",
    "            rewards = 0\n",
    "            \n",
    "            #convert the state into discrete bin values\n",
    "            digi_state = self.digitise_state(current_state)\n",
    "            \n",
    "            #close loop when either episode ends\n",
    "            while(not self.end):\n",
    "                \n",
    "                #selecting action based on q-table\n",
    "                action = np.argmax(q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3], :])\n",
    "                \n",
    "                #convert the next contininous state into discrete bin values for q table to compute \n",
    "                new_state, reward, self.end, _ ,_ = self.env.step(action)\n",
    "                new_digi_state = self.digitise_state(new_state)\n",
    "\n",
    "                                \n",
    "                #transitions agent to the next state\n",
    "                current_state = new_state\n",
    "                digi_state = new_digi_state\n",
    "                rewards += reward\n",
    "\n",
    "            #adds the reward obtained in the current episode to list containing rewards obtained in each episode\n",
    "            self.reward_episodes.append(rewards)\n",
    "            \n",
    "            #calcalates mean rewards obtained over the past 100 epsiodes\n",
    "            # mean_rewards = np.mean(self.reward_episodes[len(self.reward_episodes)-100:])\n",
    "            cumulative_rewards = np.mean(self.reward_episodes[:])\n",
    "\n",
    "            \n",
    "            #prints out updates every 10 episodes\n",
    "            if self.episode_count%2==0:\n",
    "                print(f'Episode: {self.episode_count}: {rewards}  Epsilon: {self.epsilon:0.2f}  Cumulative_Rewards {cumulative_rewards:0.1f}')\n",
    "            \n",
    "             #increment training episode\n",
    "            self.episode_count+=1\n",
    "\n",
    "        #self.plot()\n",
    "        #close the environment\n",
    "        self.env.close()\n",
    "        \n",
    "    # returns the action taken for a specific step\n",
    "    def show_discrete_action(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        #loads q table\n",
    "        f = open('cartpole.pkl', 'rb')\n",
    "        q_table = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        current_state = self.env.reset()[0]\n",
    "        digi_state = self.digitise_state(current_state)\n",
    "        \n",
    "        #selecting action with the largest q value\n",
    "        action = np.argmax(q_table[digi_state[0], digi_state[1], digi_state[2], digi_state[3], :])\n",
    "        print(\"Observation: \", current_state)\n",
    "        print(\"Chosen Action: \", action)\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.reward_episodes, label='Episode Rewards', color='b')\n",
    "        plt.title('Cumulative Reward against 100 Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.grid(False)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73885be-a46f-44c3-b440-808fcc44f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = QLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e2c36-7079-49e5-ba5b-73e09125e2d1",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fadc3ea0-018a-4952-91a7-97875098807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [-0.01412232 -0.04381574  0.01499828 -0.03553476]\n",
      "Chosen Action:  0\n"
     ]
    }
   ],
   "source": [
    "q_model.show_discrete_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97751401-cb7e-4ef4-9ad7-4b8f9f2796ce",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d25e4-fa1e-4227-8db2-1df60ff7a6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0: 866.0  Epsilon: 1.00  Cumulative_Rewards 866.0\n",
      "Episode: 2: 589.0  Epsilon: 1.00  Cumulative_Rewards 681.0\n"
     ]
    }
   ],
   "source": [
    "q_model.simulateEpisodes(100)\n",
    "q_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd99bc",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff03bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
